---
title: "SCP - Centrality"
author: ""
date: "2024-02-20"
output:
    rmdformats::readthedown:
      use_bookdown: true
      highlight: kate 
      toc_depth: 3
      number_sections: true
      code_folding: show
      css: custom.css

csl: apa.csl

params:
  id.sujeto.entrada:
    label: "Identificador del sujeto"
    value: "P0154621"
    input: "text"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Carga de librerías

```{r libraries, message=FALSE, warning=FALSE}
if (!(require(DT)))
  install.packages("DT")
library(DT)

if (!require(readxl))
  install.packages("readxl")
library(readxl)

# Rfast para matriz de varianza combinada
if (!requireNamespace("Rfast"))
  install.packages("Rfast")
library(Rfast)

if (!requireNamespace("ggplot2"))
  install.packages("ggplot2")
library(ggplot2)

if (!requireNamespace("plotly"))
  install.packages("plotly")
library(plotly)

if (!requireNamespace("ggrepel"))
  install.packages("ggrepel")
library(ggrepel)

# GridFCM
library(devtools)
if (!requireNamespace("GridFCM.practicum"))
  install_github("asanfe/GridFCM.practicum", quietly = TRUE)
library(GridFCM.practicum)

# Viridislilte
if (!requireNamespace("viridisLite"))
  install.packages("viridisLite")
library(viridisLite)

# Test para normalidad multivariante
if (!requireNamespace("MVN"))
  install.packages("MVN")
library(MVN)

if (!requireNamespace("ggpattern", quietly = TRUE))
  install.packages("ggpattern")
library(ggpattern)

if (!requireNamespace("factoextra", quietly = TRUE))
  install.packages("factoextra")
library(factoextra)

if (!requireNamespace("cluster", quietly = TRUE))
  install.packages("cluster")
library(cluster)

if (!requireNamespace("RColorBrewer", quietly = TRUE))
  install.packages("RColorBrewer")
library(RColorBrewer)

if (!requireNamespace("rcartocolor", quietly = TRUE))
  install.packages("rcartocolor")
library(rcartocolor)

```

# Importación y resumen

## Importación del objeto RDA

```{r import_RData}

# Objetos de sesión de ejemplo de la PEC
path <- '../CentralityTest/data.RData'
load(path)
samples.raw.df <- data

samples.df <- data.frame(ID = samples.raw.df$dataset$ID,
                         gender = samples.raw.df$dataset$gender,
                         age = samples.raw.df$dataset$age,
                         edu = samples.raw.df$dataset$edu,
                         status = "Error")

for(i in 1:nrow(samples.df)) {
  id <- samples.df$ID[i]  
  
  tryCatch({
    wimp <- data$grids[[id]]$WT  # Accede al wimp asociado al ID
    wphm <- GridFCM.practicum::ph_index(wimp = wimp, method = "wnorm", std = "none")

    samples.df$status[i] <- "Ok"
  }, error = function(e){
    cat("Error procesando ID:", id, "\n")
  })
}

# Calcula las frecuencias de status de procesamiento
freqs.status <- table(samples.df$status, useNA = "no")

# Calcula los porcentajes
percent.status <- prop.table(freqs.status) * 100

results.summary <- data.frame(
  ResultadoCarga = names(freqs.status),
  Casos = as.integer(freqs.status),
  Porcentaje = round(100 * prop.table(freqs.status), 3)
)

results.summary <- results.summary[, c("ResultadoCarga", "Casos", "Porcentaje.Freq")]
```

## Resultado de procesamiento

```{r import_RData_summary}

# Mostrar la tabla
DT::datatable(results.summary, options = list(pageLength = 5))

```


## Resumen de sujetos

```{r data_summary, fig.width = 10, fig.height = 10}

DT::datatable(samples.df)
```

## Wimp del sujeto a presentar

```{r wimp_import, fig.width = 10, fig.height = 10}

#path <- 'Wimp_Ejemplo.xlsx'
#opr <- TRUE
#wimp <- GridFCM.practicum::importwimp(path = path, opr = opr, sheet = 1)

# Objetos de sesión de ejemplo de la PEC
path <- '../CentralityTest/data.RData'
load(path)
samples.raw.df <- data

# Selección de un sujeto
id.sample <- params$id.sujeto.entrada
wimp <- data$grids[[id.sample]]$WT

bertin(wimp$openrepgrid, colors = c("palegreen", "darkgreen"))
```

# Exploración de la Wimp

## Digrafo del Self

```{r digraph, fig.width = 10, fig.height = 10}

# Digraph
GridFCM.practicum::digraph(wimp, layout = "rtcircle")
```

## Digrafo del Ideal

```{r idealdigraph, fig.width = 10, fig.height = 10}

# Digraph
GridFCM.practicum::idealdigraph(wimp, layout = "rtcircle")
```

## E/S de los constructos. Método Simple

```{r index_ph_test, fig.width = 10, fig.height = 10}

c.io.test <- GridFCM.practicum::degree_index(wimp, method = "simple")
c.io.test <- c.io.test[, c(2,1,3)]
c.io.test <- cbind(c.io.test, Diff = (c.io.test[, 2] - c.io.test[, 1]))
c.io.test.r <- round(c.io.test, 3)
DT::datatable(c.io.test.r)
```

## E/S de los constructos. Método wnorm

```{r in.out_wimp, fig.width = 10, fig.height = 10}

c.io.test <- GridFCM.practicum::degree_index(wimp, method = "wnorm")
c.io.test <- c.io.test[, c(2,1,3)]
c.io.test <- cbind(c.io.test, Diff = (c.io.test[, 2] - c.io.test[, 1]))
c.io.test.r <- round(c.io.test, 3)
DT::datatable(c.io.test.r)
```

## Ejemplo de salida de P-H index

```{r ph_index_test, fig.width = 10, fig.height = 10}

test.wphm <- GridFCM.practicum::ph_index(wimp = wimp, method = "wnorm", std = FALSE)
DT::datatable(test.wphm)
```

# Distancia de Mahalanobis y distribución de datos

## Test de Mardia para análisis multivariante

Llevamos a cabo previamente un test de Mardia para constrastar la normalidad multivariante de los datos, a fin de determinar la pertinencia del punto de corte basado en adecuación a distribución Chi-cuadrado de distancia de Mahalanobis

```{r normalidad_multivariante}
# Test de Mardia

test.result <- mvn(data = test.wphm, mvnTest = "mardia")

print(test.result)
  
```

## Test de resultado de la función

```{r mahalanobis_index_test, fig.width = 10, fig.height = 10}

test.wmahalanobis <- GridFCM.practicum::mahalanobis_index(wimp = wimp, method = "wnorm", std = FALSE, sign.level = 0.2)
DT::datatable(test.wmahalanobis)
```

## Distribución Chi-cuadrado

```{r chi-cuadrado, fig.width = 10, fig.height = 10}

# Definimos los grados de libertad para la distribución chi-cuadrado
df <- 2

# Generamos los valores de la distribución
x <- seq(qchisq(0.001, df), qchisq(0.999, df), length.out = 1000)
y <- dchisq(x, df)

# Calculamos los puntos de corte para el 20% superior
sign.level <- 0.2
cut_high <- qchisq(1- sign.level, df)

# Dataframe para la gráfica
data <- data.frame(x = x, y = y)

ggplot(data, aes(x = x, y = y)) + 
  geom_line() + 
  geom_ribbon(data = data %>% filter(x > cut_high), aes(ymax = y), ymin = 0, fill = 'salmon', alpha = 0.5) +
  geom_vline(xintercept = cut_high, color = "red", linetype = "dashed") +
  labs(title = 'Distribución Chi-cuadrado con puntos de corte del 80%', x = 'Valor', y = 'Densidad') +
  theme_minimal()
```

## Gráfica de barras de distancias de Mahalanobis y punto de corte

```{r barplot_mahalanobis_test, fig.width = 10, fig.height = 10}

# Distancia de Mahalanobis
test.bp.wmahalanobis <- GridFCM.practicum::mahalanobis_index(wimp = wimp, method = "wnorm", std = FALSE)
test.wmahalanobis.df <- as.data.frame(test.bp.wmahalanobis)

# Colores de los constructos


#test.wmahalanobis.df$constructo <- rownames(test.wmahalanobis)
test.wmahalanobis.df$constructo <- wimp$constructs$right.poles

# Valoración del ideal
test.wmahalanobis.df$idealdirect <- wimp$ideal$direct

# Columna para identificar constructos dilemáticos
#test.wmahalanobis.df$fill.color <- ifelse(test.wmahalanobis.df$idealdirect == 4, "yellow2", "honeydew")
test.wmahalanobis.df$fill.color <- construct_colors(wimp= wimp, mode = "red/green")

# Ordenamos las barras en orden decreciente
test.wmahalanobis.df <- test.wmahalanobis.df %>%
  arrange(desc(m.dist))

# Convertimos 'constructo' en un factor con los niveles en el orden deseado
test.wmahalanobis.df$constructo <- factor(test.wmahalanobis.df$constructo, levels = test.wmahalanobis.df$constructo)

# Punto de corte distribución Chi-Cuadrado
sign.level <- 0.2
df <- ncol(test.wphm)
chi.square.cutoff <- qchisq(1 - sign.level, df)
#media_m_dist <- mean(test.wmahalanobis.df$m.dist)
```

## Constructos supraordenados

```{r barplot_mahalanobis_test_sup, fig.width = 10, fig.height = 10}

# Crear el histograma de constructos supraordenados

# Filtramos por los constructos donde el valor de 'h' es mayor que cero
test.wmahalanobis.df.sup <- test.wmahalanobis.df %>%
  filter(h > 0)

bar_plot <- ggplot(test.wmahalanobis.df.sup, aes(x = constructo, y = m.dist, fill = fill.color)) +
  geom_bar(stat = "identity", color = "black", linewidth = 0.25) +
  geom_hline(yintercept = chi.square.cutoff, linetype = "dashed", color = "darkgreen", linewidth = 1) +
  scale_fill_identity() + # Usa los colores asignados directamente
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none" # Ocultar leyenda para fill
  ) +
  labs(x = "Constructos con h > 0", y = "Distancia de Mahalanobis", title = "Constructos con h>0 por distancia de Mahalanobis")

# Mostramos el gráfico
print(bar_plot)

```

## Constructos subordinados

```{r barplot_mahalanobis_test_sub, fig.width = 10, fig.height = 10}

# Crear el histograma de constructos subordinados

# Filtramos por los constructos donde el valor de 'h' es menor que cero
test.wmahalanobis.df.sub <- test.wmahalanobis.df %>%
  filter(h < 0)

bar_plot <- ggplot(test.wmahalanobis.df.sub, aes(x = constructo, y = m.dist, fill = fill.color)) +
  geom_bar(stat = "identity", color = "black", linewidth = 0.25) +
  geom_hline(yintercept = chi.square.cutoff, linetype = "dashed", color = "darkgreen", linewidth = 1) +
  scale_fill_identity() + # Usa los colores asignados directamente
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none" # Ocultar leyenda para fill
  ) +
  labs(x = "Constructos con h < 0", y = "Distancia de Mahalanobis", title = "Constructos con h<0 por distancia de Mahalanobis")

# Mostramos el gráfico
print(bar_plot)

```

## Distribución de valores en P

### Distribución de valores de P

```{r barplot_P, fig.width = 10, fig.height = 10}

# Crear el histograma de valores en P

test.wmahalanobis.df.sortP <- test.wmahalanobis.df %>% 
  arrange(desc(p))

mean.p <- mean(abs(test.wmahalanobis.df.sortP$p))

# Convertir 'constructo' en un factor para mantener el orden en el gráfico
test.wmahalanobis.df.sortP$constructo <- factor(test.wmahalanobis.df.sortP$constructo, 
                                          levels = test.wmahalanobis.df.sortP$constructo)

bar_plot <- ggplot(test.wmahalanobis.df.sortP, aes(x = constructo, y = p, fill = fill.color)) +
  geom_bar(stat = "identity", color = "black", linewidth = 0.25) +
  geom_hline(yintercept = mean.p, linetype = "dashed", color = "darkgreen", linewidth = 1) +
  scale_fill_identity() + # Usa los colores asignados directamente
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none" # Ocultar leyenda para fill
  ) +
  labs(x = "Constructos", y = "Valor de P", title = "Constructos por distancia en P")

# Mostramos el gráfico
print(bar_plot)

```

### Test Shapiro-Wilk (muestras pequeñas o moderadas) para variable P

```{r test_norm_uni_P}

# Test de normalidad de Saphiro-Wilk
norm.test <- shapiro.test(test.wmahalanobis.df$p)

# Imprime el resultado
print(norm.test)

p.value <- 0.05
norm.test.result <- norm.test$p.value > p.value 
```
De acuerdo con el resultado de la prueba, la **normalidad de la distribución de los valores de P es: `r norm.test.result`**

### Gráfica Cuantil-Cuantil

```{r test_graph_norm_uni_p}

datos <- test.wmahalanobis.df$p

# Gráfica q-q para comprobar la normalidad
qqnorm(datos)
qqline(datos, col = "red")
```

### Punto de corte basado en distribución normal de P

```{r dist_norm_p, fig.width = 10, fig.height = 8}

# Media y desviación típica de la distribución
mean.p <- mean(test.wmahalanobis.df$p)
sd.p <- sd(test.wmahalanobis.df$p)

# Definir el rango de valores para X basado en la media y desviación típica
x.values <- seq(from = mean.p - 4 * sd.p, to = mean.p + 4 * sd.p, length.out = 1000)

# Crear un dataframe con los valores de X y la densidad de una distribución normal para esos valores
norm.df <- data.frame(x = x.values, y = dnorm(x.values, mean = mean.p, sd = sd.p))

# Punto de corte
cut.low <- qnorm(0.15, mean = mean.p, sd = sd.p)

plot <- ggplot(norm.df, aes(x = x, y = y)) +
  geom_line() +
  geom_vline(xintercept = cut.low, linetype = "dashed", color = "red", linewidth = 1) +
  geom_vline(xintercept = mean.p, linetype = "dashed", color = "blue", linewidth = 1) +
  geom_area(data = subset(norm.df, x <= cut.low), fill = "lightblue", alpha = 0.2) +
  theme_bw() +
  theme(
    panel.grid.major = element_line(linewidth = 0.5, linetype = 'solid', colour = "lightgrey"),
    panel.grid.minor = element_blank(),
    legend.position = "none"
  ) +
  scale_x_continuous(name = "Valor de P") +
  scale_y_continuous(name = "Densidad") +
  labs(title = paste("Distribución Normal con Media en", round(mean.p, 2),
                     "y Punto de Corte en", round(cut.low, 2)))
# Mostrar la gráfica
print(plot)

```

# Representación en espacio P-H

## Sin estandarización - plotly sin marcar área no viable ni constructos centrales

```{r rep_psychlab_se_plotly, fig.height=10, fig.width=10, warning=FALSE}

wp1.grph <- GridFCM.practicum::graph_ph(wimp = wimp, method = "wnorm", std = 'none', sign.level = 0.2,
                                        mark.nva = FALSE, mark.cnt = FALSE)
wp1.grph
```

## Espacio PH con coloreado de área no útil y marcado de outliers. Función de representación

### Sin estandarización

```{r rep_mh_psychlab_se, fig.width = 10, fig.height = 10}

wp1.grph <- GridFCM.practicum::graph_ph(wimp = wimp, method = "wnorm", std = 'none', sign.level = 0.2,
                                        mark.nva = TRUE, mark.cnt = TRUE, show.points = TRUE)
wp1.grph
```

### Con estandarización basada en aristas

```{r rep_mh_psychlab_ce, fig.width = 10, fig.height = 10}

wp1.grph <- GridFCM.practicum::graph_ph(wimp = wimp, method = "wnorm", std = 'edges', sign.level = 0.2,
                                        mark.nva = TRUE, mark.cnt = TRUE, show.points = TRUE)
wp1.grph
```

### Sin estandarización, marcando área de coordenadas no viables. Sin puntos

```{r rep_psychlab_se_wimp_sp, fig.height=10, fig.width=10, warning=FALSE}

wp1.grph <- GridFCM.practicum::graph_ph(wimp = wimp, method = "wnorm", std = "none", sign.level = 0.2,
                                        mark.nva = TRUE, mark.cnt = TRUE, show.points = FALSE)
wp1.grph
```

### Sin estandarización, sin marcar área de coordenadas no viables. Sin puntos

```{r rep_psychlab_se_wimp_plotly, fig.width = 10, fig.height = 10}

wp1.grph <- GridFCM.practicum::graph_ph(wimp = wimp, method = "wnorm", std = "none", sign.level = 0.2,
                                        mark.nva = FALSE, mark.cnt = TRUE)

wp1.grph
```

# Otros métodos de centralidad

## Primer eigenvectorsobre matriz de implicaciones

```{r eigenvalues, fig.width = 10, fig.height = 10}

eigen_index <- function(wimp){

  # Matriz de adyacencia
  adj.matrix <- wimp$scores$implications

  # Vectores y valores propios
  results <- eigen(adj.matrix)

  # Extrae el primer vector propio, asociado a la varianza máxima de los datos
  eigenvector.principal <- results$vectors[,1]

  # Creamos un marco de datos con los nombres de los constructos y las componentes del primer eigenvector
  df.centrality <- data.frame(
    constructs = wimp$constructs$constructs,
    leftpoles = wimp$constructs$left.poles,
    rightpoles = wimp$constructs$right.poles,
    firsteigenvector = eigenvector.principal
  )

  return(df.centrality)
}
```

```{r test_eigenvalues, fig.width = 10, fig.height = 10}

cent.evalues.df <- eigen_index(wimp)
DT::datatable(cent.evalues.df)
```

## Análisis de componentes principales (PCA)

```{r pca, fig.width = 10, fig.height = 10}

# Foco del PCA
adj.matrix <- wimp$scores$implications

# Análisis de componentes principales
pca.result <- prcomp(adj.matrix, center = TRUE, scale = TRUE)

# Extraer los dos primeros componentes principales
pca.comp <- as.data.frame(pca.result$x[, 1:2])
pca.comp$constructs <- wimp$constructs$constructs

# Crea la gráfica de dispersión
pca.plot <- plot_ly(data = pca.comp, x = ~PC1, y = ~PC2, type = 'scatter', mode = 'markers',
                    hoverinfo = 'text+x+y',
                    marker = list(size = 10, opacity = 0.8)) %>%
            layout(title = 'PCA de matriz de adyacencia',
                   xaxis = list(title = 'PCA 1'),
                   yaxis = list(title = 'PCA 2'),
                   hovermode = 'closest',
                   plot_bgcolor = "white",
                   font = list(family = "Arial"),
                   showlegend = FALSE) %>%
            # Add annotations for each point
            add_annotations(data = pca.comp, x = ~PC1, y = ~PC2, text = ~constructs,
                            showarrow = FALSE, xanchor = 'center', yanchor = 'bottom', font = list(size = 12))

# Muestra la gráfica
pca.plot

```

# Análisis por conglomerados

## Determinación de número óptimo de conglomerados

```{r num_clusters, fig.width = 10, fig.height = 10}

k <- test_optimal_num_clusters(wimp = wimp, method = "wnorm", std = "none")
k
```

Tenemos un número máximo de **`r k` conglomerados** en nuestros datos.

## Representación de números de conglomerados óptimo

Adecuación de **cohesión** y **separación** de cada punto según pertenezca a distintos conglomerados:

```{r graph_num_clusters, fig.width = 12, fig.height = 16}

# Lista que albergará las distintas gráficas de silueta
lista.graf.sil <- list()

# Valores intermedios que ya calcula .optimal.num.clusters
max.clusters <- length(wimp$constructs$constructs) - 1
ph.mat <- GridFCM.practicum::ph_index(wimp = wimp, method = "wnorm", std = "none")
#rownames(test.dist) <- as.character(wimp$constructs$right.poles)
#distancias <- dist(test.dist, method = "euclidean")

#------------------------------
# Matriz de disimilaridad modelada como matriz de distancias de Mahalanobis
# Matriz de covarianzas
cov.matrix <- cov(ph.mat)  # Calcula la matriz de covarianza
# Vector de medias
means.vector <- colMeans(ph.mat)

# Inicializa una matriz para guardar las distancias de Mahalanobis
n <- nrow(ph.mat)
dist.mat <- matrix(NA, n, n)

# Calcula la distancia de Mahalanobis entre cada par de filas en ph.mat
for (i in 1:n) {
  for (j in i:n) {
    diff <- ph.mat[i, ] - ph.mat[j, ]
    dist.mat[i, j] <- sqrt(t(diff) %*% solve(cov.matrix) %*% diff)
    dist.mat[j, i] <- dist.mat[i, j]  # La matriz es simétrica
  }
}

# Hacemos 0 en la diagonal
diag(dist.mat) <- 0

row.names(dist.mat) <- row.names(ph.mat)
colnames(dist.mat) <- row.names(ph.mat)

#---------------------------

# Preparamos una lista con diversas representaciones gráficas de siluetas (de 2 a 10 clústeres)
for(j in 2:min(13, max.clusters)){
  it.pam <- cluster::pam(dist.mat, j, diss = TRUE)
  p <- factoextra::fviz_silhouette(it.pam, label = FALSE, print.summary = FALSE)
  lista.graf.sil[[j-1]] <- p
}

# Organizar los gráficos en una matriz de 4x3, y los presentamos
gridExtra::grid.arrange(grobs = lista.graf.sil, ncol = 3, nrow = 4)
```

### Dendrograma

```{r dendron_func, fig.height=10, fig.width=10, warning=FALSE}
#Dendrograma
dendron.plot <- constructs_dendrogram(wimp = wimp)
print(dendron.plot)
```

### ClusPlot

```{r clusters, fig.width = 10, fig.height = 9}

act.cex <- par("cex")
par(cex = 0.8)

# Calculamos el objeto de partición PAM para los k clústeres obtenidos anteriormente
opt.pam <- cluster::pam(dist.mat, k, diss = TRUE)

# Colores de los clusters
clus.colors <- carto_pal(n = k, "Peach")

cluster::clusplot(x = dist.mat,
                  clus = opt.pam$clustering,
                  shade = TRUE,
                  color = TRUE,
                  col.clus = clus.colors,
                  col.p = 'darkblue',
                  diss = TRUE,
                  labels = 3)

```
